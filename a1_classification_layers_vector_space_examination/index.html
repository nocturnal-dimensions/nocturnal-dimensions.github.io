<!DOCTYPE HTML>
<html>
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="utf-8">
    <script type="text/javascript" src="/com/jquery.js"></script>
    <script type="text/javascript" src="/com/jBox.min.js"></script>
    <link href="/com/eh.css" rel="stylesheet" type="text/css">
    <link href="/com/jBox_eh.css" rel="stylesheet" type="text/css">
	<script type="text/javascript" src='/com/MathJax/MathJax_conf.js'></script>
	<script type="text/javascript" src='/com/MathJax/MathJax.js?config=TeX-AMS_HTML-full' async></script><!-- v2.7.5 -->
    
    <meta name="Description" content="Modest Contributions / Experiments in the Field of Machine Learning and Artificial Intelligence.">
    <title>Nocturnal Dimensions</title>
  </head>
  <body>
    <div id="headbox">
		<a href="/"><div id="headtitle"> <div></div><div class="overlay"></div> </div></a>
		<div id="headabout_anchor"><div id="headabout_box"><div id="headabout_arrow"></div><div id="headabout_text"></div></div></div>
	</div>
	<div id="headline"><div>God Is a Vector</div></div>
	<span class="reference"></span>
	<div id="aboutbox">
		<div id="aboutcontent">
			<div id="aboutportrait"></div>
			<div id="aboutarrow_push"></div><div class="aboutarrow" id="aboutarrow_in"><div></div></div>
			<div id="abouttext">
			<span class="com"></span>
			<span style="line-height: 1.2">
			Thank you for the visit. My name is Dmitry Tabakov.
			This site is a personal sandbox for my modest contributions / experiments in the fields of machine learning and artificial intelligence.
			Author would like to describe himself as a garage researcher and hobbyist math addict. Also I am one of the mentors for the <a target="_blank" href="https://www.coursera.org/specializations/deep-learning">DeepLearning.ai specialization</a>.
			Reach me at <i>nervendsinpowerlines@protonmail.ch</i>
			<br><br>
			Also author feels like he needs to appologize in advance for being quite a lazy reader at times and, thus, a possibility of some of the described ideas being brought up elsewhere in a more appealing manner.
			But at least I try to be original here besides the content I explicitly refer to.
			The site's licensed under <a href="/LICENSE.txt" target="_blank">Creative Commons Attribution 4.0</a>.
			In case you find something useful I'd appreciate to see my articles referenced in your paper / work.
			</span>
			</div>
			
			<div style="clear:both;"></div>
		</div>
		<div class="aboutarrow" id="aboutarrow_out"><div></div></div>
	</div>
	<div id="cbox">
<h2>Classification Layer's Vector Space: Part I - Examination</h2>
<p class="abstract">
In a classification <span class="tooltip">NN</span><ins>Neural Network</ins> of any complexity or structure the last layer's weights and activation function define a specific vector space, which attributes any activation of the previous layer to a certain class.
In this introductory article an example of such space is scrutinized using the multiclass Softmax last layer, which is a common classification NN setup.
We examine the basic facts about the aforementioned space and illustrate it for a clearer understanding of how it functions. The facts discussed here also apply to general multinomial regression.
</p>

<div class="fold h4">
<div>Notation Reference<pre><div></div><div></div></pre></div>
<div>
<p>
Since there is no universal notation conventions for NN models, this section briefly clarifies the definitions that are used in this and further articles.
Consider a basic fully connected NN model with <em>L</em> layers, then the following scheme illustrates the <span class="tooltip">whole</span><ins>only the <em>[1]</em>st layer is shown in detail, but all other layers are alike</ins> model structure:
</p>
<div class="img" id="img1"><div style="padding-bottom: 34.27%">
<object data="notation_scheme.svg" type="image/svg+xml"><img src="notation_scheme.svg" alt="basic neural network model" /></object>
</div>
<pre></pre>
</div>
<div class="col">
<ul>
<li>Superscript <em>[l]</em> denotes a quantity associated with the \(l^{th}\) layer</li>
<li>\(m\) is the number of training samples</li>
<li>\(n^{[l]}\) is the <span class="tooltip">number of units</span><ins>which is the dimensionality of layer <em>l</em>'s activations</ins> in layer <em>l</em></li>
<li>Matrix \(\mathbf{X}\) is the training set data with the <span class="tooltip">first dimension</span><ins>rows of the matrix</ins> containing individual training samples</li>
<li>Matrices \(\mathrm{A}^{[l]}\) contain <span class="tooltip">activations</span><ins>which are final outputs of layer <em>l</em></ins> of each layer <em>l</em>. <span class="tooltip">First dimension</span><ins>rows of a matrix</ins> of these matrices contain latent vector representations for each training sample in the layer <em>l</em>. Matrix \(\mathbf{X}\) can also be called \(\mathrm{A}^{[0]}\) and \(n^{[0]}\) is the dimensionality of <span class="tooltip">training samples</span><ins>for instance the number of pixels in a picture</ins></li>
<li>Matrices \(\mathrm{W}^{[l]}\) contain <span class="tooltip">weights</span><ins>which are variables subject to optimization in NNs</ins> of each layer <em>l</em> and are used in matrix multiplication <span class="tooltip">(\(\cdot\))</span><ins>also denotes simple scalar vector products</ins> with \(\mathrm{A}^{[l-1]}\) to compute \(\mathrm{Z}^{[l]}\)</li>
<li>\(g^{[l]}()\) is a non-linear activation function in layer <em>l</em>, which is applied to \(\mathrm{Z}^{[l]}\) to compute activations \(\mathrm{A}^{[l]}\)</li>
<li>Lowercase letters \(x, a, w, z\) are used to represent individual elements of their respective upper case matrices when used with <span class="tooltip">2 lowerscript indices</span><ins>or more in some models</ins>. When these letters are used with only one index, then they represent a <span class="tooltip">full row</span><ins>or an entry of the first dimension in other words. It is colored <span class="cr">red</span> on the scheme above</ins> of their respective matrix. And without any lowerscript indexes they simply refer to <span class="tooltip">any row</span><ins>or data that can potentially be a row of the respective matrix</ins> of the matrix</li>
<li>Vectors \(b^{[l]}\) contain <span class="tooltip">weight biases</span><ins>which are also subject to optimization in NNs</ins>. Individual elements of these vectors, however, are called \(w_{1,0}\dots w_{n^{[l]},0}\) because vector \(b^{[l]}\) can be thought of as an <span class="tooltip">extra column</span><ins>indeed in some specifications it is included in matrix \(\mathrm{W}^{[l]}\), for instance, if homogeneous coordinates are used for \(\mathrm{A}^{[l-1]}\)</ins> of the matrix \(\mathrm{W}^{[l]}\)</li>
</ul>
</div>
<div class="col">
<p>
However, in the series of articles "Classification layer's vector space" we are going to overlook only the very last layer <em>L</em> of a NN with the activation function \(g^{[L]}=\mathrm{softmax(Z}^{[L]})\).
And, thus, for simplicity we exclude the superscript indexes <em>[l]</em> from all the variables, since most of them would refer to the layer <em>L</em>. Except matrix \(\mathrm{A}^{[L-1]}\), which is the input to the layer <em>L</em> generated by the previous layer - we will be calling this matrix \(\mathrm{X}\) or <em>x</em> for individual rows here, which also emphasizes that we are looking at them as <span class="tooltip">independent variables</span><ins>In fact they depend on the weights of previous layers and training data but for the purpose of this article we pretend that they are independent inputs (just like when the backpropagation chain rule is applied it is also pretended at some step that \(\mathrm{A}^{[L-1]}\) is independent to find its intermediate gradient</ins> in these articles, just like if it was the initial training set data.
<span class="tooltip">\(n^{L-1}\)</span><ins>dimensionality of the inputs <em>x</em></ins> will be simply denoted as <em>n</em>, while <span class="tooltip">\(n^{L}\)</span><ins>dimensionality of the output activations <em>a</em></ins> will be called <em>N</em>, which is at the same time the number of possible classes.
</p>
</div>
</div>

</div>

<h4>Softmax layer intro</h4>
<div class="col">
<p>
When <span class="tooltip">plain natural data</span><ins>an image or a sound wave</ins> is feeded to a NN its purpose is to convert the data from a complex non-linear form to a simplier and "more linear" representation for that data by applying linear and non-linear transformations through a set of logical constructs called layers.
At a certain deep layer (typically <em>L-1</em>) a data sample achieves its simpliest, abstract but unclear for people vector <span class="tooltip">representation</span><ins>which depending on the context can be refered to as <i>latent variable</i> or <i>features</i> or <i>embedding</i></ins> for that model<span class="ref">[4]</span>. And then the latent representation can be used in various useful ways depending on the task of the model. In this article we consider a classification task, where initial data samples should be attributed to their respective class or type.
A common way of setting up such model is to add the last layer with the \(\mathrm{softmax}()\) activation function, which translates latent representations into estimates of probabilities that a data sample belongs to each of the possible classes. So each data sample feeded to the NN is given its own probabilities vector which is passed to the cost function at the final stage, subject to optimization.
</p>
<p>
Despite not easily explicable internal mechanics of a NN, one important thing that should be understood here is that <span class="important">softmax layer is capable of classifying not only representations generated from the training or validation data, but actually just any vector <em>x</em> with appropriate dimensionality</span>. In that sense softmax layer creates a special kind of space, where every point of this space is associated with a <span class="tooltip">class</span><ins>more strictly with a distribution over possible classes, since as mentioned above softmax generates a vector of probabilities</ins>.
</p>
<p>
In these series of articles we will study the properties of this space. Though, unlike works which study semantic properties of word vector representations (for instance inspiring papers by Mikolov et al.<span class="ref">[1]</span>, which demonstrated vector arithmetics for discovering word similarities), we focus more on mathematical properties here. A similar space is formed by <i>Support Vector Machines</i>, which is another popular alternative to softmax in classification tasks<span class="ref">[5]</span>, though we will not discuss it here specifically.
</p>

<p>
So as mentioned above softmax layer produces vectors of estimated probabilities for each input vector <em>x</em>. Such vectors are stored as rows of activation matrix \(\mathrm{A}\) for each input <em>x</em> and can be written explicitly as
$$\begin{equation}a = (\frac{e^{w_1 \cdot x + w_{1,0}}}{\sum_{j=1}^{N}e^{w_j \cdot x + w_{j,0}}},\; \dots \;, \frac{e^{w_N \cdot x + w_{N,0}}}{\sum_{j=1}^{N}e^{w_j \cdot x + w_{j,0}}})\end{equation} \label{r1}$$
Each element of vector <em>a</em> can be refered to as softmax function for a class <em>i</em> or \(\mathrm{softmax}_i(x)\), since it's an estimation of probability that <em>x</em> belongs to the corresponding class \(i=1\dots N\). Note that the sum of elements of vector <em>a</em> always equals to <b>1</b> by design, which guarantees that the estimated probabilities are a valid discrete probability distribution.
Exponential functions also provide that the values will be greater than zero. The arguments to the exponential functions are elements of vector <em>z</em>, which is the linear part of the layer:
$$\begin{equation}z = (w_1 \cdot x + w_{1,0},\;\dots\;,w_N \cdot x + w_{N,0})\end{equation}\label{r2} $$
</p>
<p>
<em>z</em> consists of simple <i>scalar products</i> of weight vectors \(w_1\dots w_N\) with the vector <em>x</em> plus biases.
It can be noticed that function <span class="important">\(\mathrm{softmax}_i(z)\) is monotonic in respect to every element of vector <em>z</em></span>.
Indeed, elements of vector <em>z</em> can be divided into 2 groups: the ones that are present only in the denominator of \(\mathrm{softmax}_i\) and the single one that is included both into numerator and denominator, which we will be calling <span class="tooltip">the key element</span><ins>it is also the \(i^{th}\) element of vector <em>z</em></ins>.
The first group elements are monotonically decreasing for \(softmax_i\), because any gain in denominator reduces fraction's value. Any gain in the key element, on the other hand, forces \(\mathrm{softmax}_i\) to incease, because with other components of denominator staying put this gain reduces the relative difference between numerator and denominator.
In other words, when the \(i^{th}\) element of a vector <em>z</em> is increased with everything else unchanged, then the probability of the \(i^{th}\) class also increases, but falls for all other classes.
Considering this fact and despite the non-linear nature of the softmax activation, we can learn some basics of the softmax's layer vector space by studying only vector <em>z</em> itself, which is done in the next section.
</p>
</div>

<h4>Linear Space</h4>
<div class="col arrow">
<p>
In this part we examine the space formed by vector <em>z</em> specified in \eqref{r2}, which is simplier to start with and at the same time is highly related to the full activation vector <em>a</em>. 
</p>
<p>
As it was discussed in the previous section probability of \(i^{th}\) class increases only when the value of \(i^{th}\) <i>key element</i> of <em>z</em> increases. Since <em>x</em> is given for this layer, then value of \(i^{th}\) element can be altered only by vector \(w_i\) and bias \(w_{i,0}\). So we can conclude that <span class="important"><span class="tooltip">vector \(w_i\)</span><ins>\(i^{th}\) row of matrix \(\mathrm{W}\)</ins> and bias \(w_{i,0}\) are <b>defining</b> for the class <em>i</em></span>.
Scalar product \(w_i \cdot x\) equals \(|w_i| |x| \cos(w_i, x)\) according to the geometric definition of the scalar product, and considering this representation we can make a judgement that the direction of vector <em>x</em> and \(w_i\) should <span class="tooltip">approximately coincide</span><ins>because it gives higher values of \(\cos(w_i, x)\)</ins> for the probability of class <em>i</em> to be high.
</p>
<p>
However, to get a thorough understanding of what inputs <em>x</em> induce high values for each key element of vector <em>z</em> we can use <span class="tooltip">contours graphs</span><ins>this graph is similar to elevation maps used in cartography, where lines denote one elevation level and colors are also used to distinguish low and high elevations. In our case only elevations are replaced by the values inside vector <em>z</em></ins> for the elements of vector <em>z</em>.
The graph depicts all possible points <em>x</em> and so it is an <em>n</em>-dimensional graph. Also vectors \(w_i\) can be depicted on the same graph since they have the same dimensionality as <em>x</em> and serve as the reference direction for the class that they define. We start with this step:
</p>
<div class="img" id="img2"><div style="padding-bottom: 100%">
<object data="a1_vecs.svg" type="image/svg+xml"><img src="a1_vecs.svg" alt="illustration for vectors w" /></object>
</div>
<p>figure 1 - graphic representation for vectors \(w_i\)</p>
<pre></pre>
</div>
<p>
<span class="scroll" data-target="#img2">Figure 1</span> illustrates one particular weight matrix
$$\mathrm{W} = \begin{pmatrix}
1 & 2\\ 
-2 & 1\\ 
0.2 & -1
\end{pmatrix}$$
which specifies a softmax layer with the dimensionality of inputs <em>n=2</em> and the number of classes <em>N=3</em>. We will be using this example for all further illustrations.
Even though such setup is feasible, it is important to note that normally in real NN models <em>n>N</em> and <em>n</em> is greater than 20, which increases performance.
Our example is artificial, so that we can depict vectors in a 2-dimensional graph, however, it is good enough for our purpose as the main properties are kept and at the same time it is clear how to extend this space to higher dimensions.
</p>
<p>Now we start adding contours for the <span class="tooltip">elements of <em>z</em></span><ins>as they are responsible for the probability of their respective class, we will use its own color for each group of contours to emphasize its attribution to a particular class. So colors on the graph are associated with classes</ins>, assuming that bias vector <em>b</em> is zero:</p>
<div class="img" id="img3"><div style="padding-bottom: 79.36%">
<object data="a1_contours.svg" type="image/svg+xml"><img src="a1_contours.svg" alt="contours of z" /></object>
</div>
<p>figure 2 - contours of <em>z</em></p>
<pre></pre>
</div>
<p>
As you can see, the contours for each class are <span class="tooltip">perpendicular lines</span><ins>it means that all points <em>x</em> on one line are such that they yield the same value for the respective scalar product \(w_i \cdot x\)</ins> to the defining vectors \(w_i\).
The more intense is the color, the greater is the value of the respective element of <em>z</em>. All values that are below zero are rendered completely transparent to prevent overlapping of contours everywhere, but keep in mind that values also vary there for each class.
Areas with an intense color, which doesn't overlap with another intense color are those, which give high probability to one class and will be called <i>clean</i>. Areas where at least 2 colors overlap are <b>dirty</b> and will give some mixed probability distribution. Common classification NN searches only for the first type of points to assign one class to one data sample. So the first conclusion that we can make here is that <span class="important">softmax's vector space contains a lot of mixed-probability and, thus, useless areas</span>.
The second fact to note is that <span class="important">the pairwise angles between vectors \(w_i\) must be wide enough for the clean areas to occupy larger space</span>. For instance the clean area for \(\cbr{{w_3}}\) is relatively larger <span class="scroll" data-target="#img3">(fig. 2)</span>, due to \(\cbr{{w_3}}\) having greater angles with \(\cbg{{w_1}}, \cby{{w_2}}\).
</p>
<p>Next we take a look at how basic transformations affect this space:</p>
</div>

<div class="col internal">
<div class="img" id="img4"><div style="padding-bottom: 71.42%; width: 71.42%">
<object data="a1_scale.svg" type="image/svg+xml"><img src="a1_scale.svg" alt="effect of scaling on linear space" /></object>
</div>
<p>figure 3 - effect of scaling <em>w</em> in linear space</p>
<pre></pre>
</div>
<div class="img" id="img6"><div style="padding-bottom: 71.42%">
<object data="a1_const.svg" type="image/svg+xml"><img src="a1_const.svg" alt="effect of biases on linear space" /></object>
</div>
<p>figure 4 - effect of bias weights <em>b</em> in linear space</p>
<pre></pre>
</div>
</div>
<div class="col">
<p>
In <span class="scroll" data-target="#img4">figure 3</span> the length of the vectors \(w_i\) is being altered, while the direction's kept. It results in scaling of contours along the defining vectors.
As can be seen, the length of weights also affects the size of the clean area for their classes. For instance, when \(\cbr{{w_3}}\) is twice as short as 2 other vectors its clean area has dim color, meaning that input vector <em>x</em> on the contrary <span class="tooltip">must be long</span><ins>or in other words less points <em>x</em> belong to the \(3^{rd}\) class with a high probability, thus, the clean area is smaller</ins> to hit area with the intense <span class="cbr">red</span> color.
</p>
<p>
<span class="scroll" data-target="#img6">Figure 4</span> demonstrates the effect of changing the bias vector <em>b</em>. Biases translate contours along vectors \(w_i\) in the <span class="tooltip">opposite direction</span><ins>positive bias \(w_{i,0}\) shifts contours in the opposite direction of vector \(w_i\), and vice versa</ins>.
So biases are another mean of controlling clean areas by linearly shifting them.
However, the strength of the translation will again depend on the length of vector \(w_i\): note that in the <span class="scroll" data-target="#img6">figure 4</span> both offsets \(w_{1,0} \text{ and } w_{3,0}\) have the same absolute change <span class="tooltip">2</span><ins>though the direction is opposite</ins>, but on the graph <span class="cbr">red countours</span> are moved much further compared to the <span class="cbg">green contours</span> and it is due to \(\cbg{{w_1}}\) being longer than \(\cbr{{w_3}}\).
</p>
<p>These effects will be clarified further yet in the next section.</p>
</div>




<h4>Softmax Layer's Class Space</h4>
<div class="col arrow">
<p>
In the previous section we have examined an intermediate space for the linear vector <em>z</em>. Now we will add the non-linear \(\mathrm{softmax}\) activation to <em>z</em>, which converts it to the vector of probabilities <em>a</em> \eqref{r1}.
We again use the <i>contours graph</i> to depict this space and again the contours are plotted for each element of the vector <em>a</em> with its own color and \(b=0\):
</p>
<div class="img" id="img7"><div style="padding-bottom: 79.36%;">
<object data="a1_soft_contours.svg" type="image/svg+xml"><img src="a1_soft_contours.svg" alt="softmax space embedding" /></object>
</div>
<p>figure 5 - contours of <em>a</em></p>
<pre></pre>
</div>
<p>
This picture can be thought of as just a specific transformation of the previous one, which it is in fact. The color intensity this time is used to differentiate probabilities, and the points with the highest probabilities per class are drawn with the brightest colors. Areas where probabilities are below 50% are transparent for each set of contours, which ensures that contours <span class="tooltip">don't intersect</span><ins>because if one class has at least 50% probability then no other class can reach 50% in the same point (note that probabilities can't equal to zero too, they only approach it due to softmax formula)</ins> and helps to identify clean areas with one dominant class.
Thus, the center point of the graph is rendered completely transparent as all classes there have only 33% probability.
</p>
<p>
Area which is associated with the <span class="cbr">\(3^{rd}\) class</span> looks almost the same as 2 other areas, because as mentioned before vector \(\cbr{{w_3}}\) has wide angles with 2 other vectors, which increases the area but at the same time vector \(\cbr{{w_3}}\) is shorter than other 2, which decreases the area. Overall, these 2 differences compensate each other.
</p>
<p>
Another important thing to note, is that contours are not perfectly aligned to the defining vectors \(w_i\) anymore, though their directions are still related. But there are new crucial geometrical dependencies, which we will highlight next while trying to alter biases <em>b</em>. So let's have a look at what they do:
</p>
</div>
<div class="col arrow">
<div class="img" id="img8"><div style="padding-bottom: 71.42%;">
<object data="a1_soft_const.svg" type="image/svg+xml"><img src="a1_soft_const.svg" alt="effect of biases on softmax" /></object>
</div>
<p>figure 6 - effect of bias weights in class space</p>
<pre></pre>
</div>
<p>
One single bias now has an effect on all the contours unlike before, because it is included in all the elements of <em>a</em>. However, all the contours are still moving linearly. While it is quite obvious from the picture, we still need to find a mathematical justification.
For simplicity let's consider only the <span class="cbg">\(1^{st}\) class</span> contours and so the first element of <em>a</em>: \(\frac{e^{w_1 \cdot x + w_{1,0}}}{\sum_{j=1}^{N}e^{w_j \cdot x + w_{j,0}}}\). If we assume that altering \(w_{1,0}\) results in a linear move of the contours, then we will be able to neutralize this move by shifting vector <em>x</em> by some unknown vector <em>c</em>, which is a standard way of translating a graph:
$$
\begin{align}
&\bullet \frac{e^{w_1 \cdot (x+c) + w_{1,0}}}{\sum_{j=1}^{N}e^{w_j \cdot (x+c) + w_{j,0}}} = \quad\text{(divide by numerator)}\notag\\
&\circ = \frac{1}{1 + \sum_{j\neq 1} e^{(w_j-w_1) \cdot (x+c) + w_{j,0} - w_{1,0}}}\label{ref100}\end{align}
$$
now we show that the arguments of the exponential functions can equal to what they were before adding <em>c</em> and \(w_{1,0}\)
$$
\bullet \left\{\begin{matrix}
{(w_2-w_1) \cdot (x+c) + w_{2,0} - w_{1,0}} = {(w_2-w_1) \cdot x + w_{2,0}} \\
{(w_3-w_1) \cdot (x+c) + w_{3,0} - w_{1,0}} = {(w_3-w_1) \cdot x + w_{3,0}}
\end{matrix}\right.
$$
$$
\bullet \left\{\begin{matrix}
{(w_2-w_1) \cdot x + (w_2-w_1) \cdot c - w_{1,0}} = {(w_2-w_1) \cdot x } \\
{(w_3-w_1) \cdot x + (w_3-w_1) \cdot c - w_{1,0}} = {(w_3-w_1) \cdot x }
\end{matrix}\right.
$$
$$
\begin{equation}
\circ \left\{\begin{matrix}
(w_2-w_1) \cdot c = w_{1,0} \\
(w_3-w_1) \cdot c = w_{1,0}
\end{matrix}\right.
\end{equation}\label{ref101}
$$
By solving the last system of equations such <em>c</em> can be found and this <em>c</em> will be not dependent on <em>x</em> as it is not even present in the system, so this <em>c</em> will be able to neutralzie the effect of \(w_{1,0}\) completely.
We may also multiply this <em>c</em> by -1 if we want to make it replicate the effect of \(w_{1,0}\) instead.
And if we consider other elements of <em>a</em> and write the analogous system of equations to neutralize the \(w_{1,0}\) bias we will find out that it will be the same system of equations after some transformations, meaning that <span class="important">\(w_{1,0}\) translates all the contours equally</span>.
</p>
<p>
The system has 2 equations and <span class="tooltip">2 unknown variables</span><ins>because <em>c</em> is 2 dimensional in our example</ins> and the system is linear, therefore, only one <em>c</em> exists which satisfies it.
But in a general softmax layer there will be <span class="tooltip"><em>N-1</em> equations</span><ins>Because there are <em>N-1</em> exponential functions in the denominator \eqref{ref100}</ins> and <em>n</em> unknown variables, so at least one solution exists and <span class="important">bias weights <em>b</em> translate the space linearly only if \(N\leq n+1\)</span>. And if \(N\gt n+1\), then the biases <em>b</em> transform the space in a more complicated way. But as was mentioned before in most real models <em>n</em> is substantially greater than <em>N</em> and so the linear translation property stays true.
If \(N\lt n+1\) then multiple solutions <em>c</em> exist and the translation is linear still, but how is it possible? The subtlety is that the contours will be <i>invariant to some directions</i> and so multiple translation directions exist, which all result in the <span class="tooltip">same transformation</span><ins>for instance, if we had <em>N=2</em> in our example then the contours would be lines and lines can be translated in multiple directions but with the same result</ins> of the contours.
</p>
<p>
The equations system for finding <em>c</em> \eqref{ref101} doesn't have a solution, which can be represented by a simple formula, but the intuition behind the solution is the following:
</p>
<p class="important">
To find the translation directions for a bias \(w_{i,0}\):<br>
1. Construct a <span class="tooltip">hyperplane</span><ins><em>N-2</em>-dimensional</ins> passing through all the defining vectors \((w_{1},\;\dots,\;w_{N})\) but vector \(w_{i}\)<br>
2. Find all the perpendicular directions to that hyperplane, which will form a <span class="tooltip">translation plane</span><ins><em>(n-N+2)</em>-dimensional</ins>
</p>
<p>
In our particular case hyperplane has only one dimension, meaning that it is simply a line connecting \(w_3\) and \(w_2\). And a vector perpendicular to this line will be the translation direction for \(w_{1,0}\).
In the <span class="scroll" data-target="#img8">figure 6</span> this direction is clearly visible as the narrow white line between the <span class="cbr">red</span> and the <span class="cby">yellow</span> area.
</p>
<p>To make all the mentioned facts clear let's yet illustrate them with a similar example but where <em>n=3</em>:</p>

<div class="img" id="img9"><div style="padding-bottom: 100%;">
<object data="a1_3d.svg" type="image/svg+xml"><img src="a1_3d.svg" alt="softmax contours for 3 classes in 3D space" /></object>
</div>
<p>figure 7 - Softmax contours for <em>N=3</em> and <em>n=3</em></p>
<pre></pre>
</div>
<p>
Unlike before <span class="scroll" data-target="#img9">Figure 7</span> has contours only for the 50% probability levels depicted, but they <span class="tooltip">encompass the volumes</span><ins>In other words they are the boundaries of the clean areas</ins>, which are the clean areas for their respective classes.
Blue checkered plane is the translation plane of the graph for the bias \(w_{1,0}\). To construct it we needed to draw a line between \(w_2\) and \(w_3\) <span class="tooltip">again</span><ins>as the number of classes <em>N</em> didn't change</ins> and then find all the directions which are perpendicular to that line.
Even though there are multiple directions inside the plane - they all can lead to the same <span class="tooltip">translation result</span><ins>though the distance should be chosen appropriately</ins>, since there is also one <i>invariance direction</i> for this case.
</p>
<p>
<span class="important">Invariance directions</span> are such that <span class="important">do not change</span> the values of functions, and in our particular case <span class="important">the output probability distribution</span>. Following this definition we can make a system of equations to identify such directions.
Consider we have any starting point <em>x</em> and we want to shift it in one direction <em>c</em> but in such a way that the <span class="tooltip">value of the softmax function</span><ins>let's consider the value of the first element of <em>a</em> here but it doesn't really matter as all of them yield the same system of equations again</ins> do not change. Then from \eqref{ref100} we have the following system:
<span style="font-size:0.85em">
$$
\bullet \left\{\begin{matrix}
{(w_2-w_1) \cdot (x+c) + w_{2,0} - w_{1,0}} = {(w_2-w_1) \cdot x + w_{2,0} - w_{1,0}} \\
{(w_3-w_1) \cdot (x+c) + w_{3,0} - w_{1,0}} = {(w_3-w_1) \cdot x + w_{3,0} - w_{1,0}}
\end{matrix}\right.
$$
</span>
$$
\begin{equation}
\circ \left\{\begin{matrix}
(w_2-w_1) \cdot c = 0 \\
(w_3-w_1) \cdot c = 0
\end{matrix}\right.
\end{equation}\label{ref102}
$$
</p>

<p>
As you may notice the systems \eqref{ref101} and \eqref{ref102} are very related, and thus the intuition for the solution to \eqref{ref102} also will be similar:
</p>
<p class="important">
To find the invariance directions for a softmax layer:<br>
1. Construct a <span class="tooltip">hyperplane</span><ins><em>N-1</em>-dimensional</ins> passing through all the defining vectors \((w_{1},\;\dots,\;w_{N})\)<br>
2. Find all the perpendicular directions to that hyperplane, which will form an <span class="tooltip">invariance plane</span><ins><em>(n-N+1)</em>-dimensional</ins>
</p>
<p>
<span class="important">Invariance directions exist only if <span class="tooltip">\(N\leq n\)</span><ins>And that's why there are no such directions in our main case <span class="scroll" data-target="#img8">(fig. 6)</span></ins></span>.
In <span class="scroll" data-target="#img9">Figure 7</span> all invariance directions belong to one line only but in a general case they form an <em>(n-N+1)</em>-dimensional <i>invariance plane</i>.
That plane is of an utter importance, because it can be used to code <span class="tooltip">extra information</span> into the vector representations without affecting the distribution of probabilities for the classification task.
<ins>For instance, if <span class="scroll" data-target="#img9">Figure 7</span> was an illustration to a classificator for images of animals of 3 types: dogs, cats and birds, then the invariance line could be used to encode the color of an animal, say, from black to white. Obviously, we would also want to have more than one dimension for coding extra infromation, like the size and the position of an animal, its age and other features, so it would be good to choose a larger <em>n</em></ins>
That's crucial to consider for getting robust vector representations and that's also why it is <span class="tooltip">useful to choose <em>n &gt;&gt; N</em></span><ins>If invariance plane is learned correctly and NN learns to map data samples to that plane correctly along with the main classification task, then this actually should improve the classification performance on the test set too, because in a sense it filters out correctly the features, which are present in the data sample, but are not directly related to the classification task, and so it leaves less space for making a mistake in the classification task itself. So learning such mapping correctly is tightly related to reducing the overfitting problem</ins>.
</p>


<p>
<span class="scroll" data-target="#img8">Figure 6</span> also emphasizes that biases \((w_{1,0},\;\dots,\;w_{3,0})\) are stackable and the contours can be translated linearly in some combined direction. In fact, it appears that:
</p>
<p class="important">
In any softmax layer setup any linear translation of the space is achievable with the bias vector <em>b</em>.
</p>
<p>
It is easy to see if you consider a system like \eqref{ref101} again, but with the shift vector <em>c</em> given and the biases \((w_{1,0},\;\dots,\;w_{N,0})\) as unknown variables instead. Then in such case that system has <em>N</em> unknown variables and <em>N-1</em> equations, meaning that there always exists a solution.
Note that it is true even when \(N > {n + 1}\): though <em>b</em> is capable of more complex transformations as was mentioned earlier it still allows linear translations too.
Overall, <span class="important">bias vector <em>b</em> can be thought of as a way to adjust the class space without changing its shape</span>.
</p>
<p>Next, similarly to the linear space we take a look at how scaling the defining vectors affects the space.</p>
</div>


<div class="col arrow">
<p>
To understand the effect of scaling the weights we follow the same logic as with the biases case - try to neutralize the scaling factor <em>s</em> completely with some space transformation.
But this time simply translating the space with a shift vector <em>c</em> is not enough, and instead we have to apply a more general <i>affine transformation</i> by multiplying input <em>x</em> by some <span class="tooltip">transformation matrix <em>S</em></span>.
<ins>Matrix <em>S</em> must be square so that the dimensionality of <em>x</em> is kept. More on affine transformations <a href="https://en.wikipedia.org/wiki/Affine_transformation" target="_blank">here</a></ins>
</p>
<p>
Let's again consider only the first element of <em>a</em> and only the first defining vector \(w_{1}\) being scaled by the factor <em>s</em>:
<span style="font-size:0.9em">$$
\begin{align}
&\bullet \frac{e^{s w_1 \cdot (S\cdot x) + w_{1,0}}}{e^{s w_1 \cdot (S\cdot x) + w_{1,0}} + \sum_{j=2}^{N}e^{w_j \cdot (S\cdot x) + w_{j,0}}} = \quad\text{(divide by numerator)}\notag\\
&\circ = \frac{1}{1 + \sum_{j\neq 1} e^{(w_j-s w_1) \cdot (S\cdot x) + w_{j,0} - w_{1,0}}}\label{ref103}\end{align}
$$</span>
From \eqref{ref103} we make a system of equations to reduce <em>S</em> and <em>s</em> without changing the value of the function:
<span style="font-size:0.85em">
$$
\bullet \left\{\begin{matrix}
{(w_2-s w_1) \cdot (S\cdot x) + w_{2,0} - w_{1,0}} = {(w_2-w_1) \cdot x + w_{2,0} - w_{1,0}} \\
{(w_3-s w_1) \cdot (S\cdot x) + w_{3,0} - w_{1,0}} = {(w_3-w_1) \cdot x + w_{3,0} - w_{1,0}}
\end{matrix}\right.
$$
</span>
$$
\bullet \left\{\begin{matrix}
{((w_2-s w_1) \cdot S)\cdot x} = {(w_2-w_1) \cdot x } \\
{((w_3-s w_1) \cdot S)\cdot x} = {(w_3-w_1) \cdot x }
\end{matrix}\right.
$$
$$
\bullet \left\{\begin{matrix}
{((w_2-s w_1) \cdot S - (w_2-w_1))\cdot x} = 0 \\
{((w_3-s w_1) \cdot S - (w_3-w_1))\cdot x} = 0
\end{matrix}\right.
$$
The last system is true for any <em>x</em> only when:
$$
\bullet \left\{\begin{matrix}
{(w_2-s w_1) \cdot S - (w_2-w_1)} = \vec{0} \\
{(w_3-s w_1) \cdot S - (w_3-w_1)} = \vec{0}
\end{matrix}\right.
$$
$$
\begin{equation}
\circ \left\{\begin{matrix}
{(w_2-s w_1) \cdot S} = w_2-w_1 \\
{(w_3-s w_1) \cdot S} = w_3-w_1
\end{matrix}\right.
\end{equation}\label{ref104}
$$
</p>
<p>
Both sides of the equations in \eqref{ref104} are vectors, meaning that every pair of corresponding elements of these vectors must be equal. That is why the actual number of equations in such system is <span class="tooltip">\((N-1)\times n\)</span><ins>4 in our example</ins>.
Matrix <em>S</em>, which is to be found, has the dimensions \(n \times n\). Since the equations in \eqref{ref104} are linear <span class="important">the system has at least one solution when \(N\leq n+1\)</span> again. And if <em>N>n+1</em> then matrix <em>S</em> is not sufficient to neutralize the scaling factor <em>s</em>.
</p>
<p>
After matrix <em>S</em> is found from the system \eqref{ref104} its <i>inverse matrix</i> should be used if we want to make it replicate the effect of the factor <em>s</em> instead of neutralizing it.
Also similarly to the situation with biases the system of equations and its satisfying matrix <em>S</em> appears to be the same for all other elements of <em>a</em> too, meaning that <span class="important">a single scaling factor <em>s</em> for one defining vector affects all the contours equally</span>.
</p>
<p>
The solution matrix <em>S</em> is not easy to interpret but generally all the elements of <em>S</em> will be non-zero and such space transformation is called <i>scaling and shear</i>.
</p>
<div class="img" id="img10"><div style="padding-bottom: 100%;">
<object data="a1_soft_scale.svg" type="image/svg+xml"><img src="a1_soft_scale.svg" alt="effect of scaling weights on softmax" /></object>
</div>
<p>figure 8 - effect of scaling <em>w</em> in class space</p>
<pre></pre>
</div>
<p>
The primary effect of scaling the weights, as can be seen in <span class="scroll" data-target="#img10">figure 8</span>, is relatively increasing/decreasing the size of clean areas: <span class="important">the longer is the defining vector the <span class="tooltip">larger</span><ins>in terms of the percentage of the whole available space</ins> is its corresponding clean area in expense of the size of other clean areas nearby and vice versa</span>.
</p>
<p>
Interestingly, when <em>s=0</em> <span class="tooltip">the weight vector becomes \(\vec{0}\)</span><ins>shown for \({w_2}\)</ins>, but its clean area is still retained and there are points in space, where the probabilities for its class are very high still.
It happens, because exponential functions inside the softmax function always give 1 in case of a zero defining vector no matter what point of space is chosen, however other exponential functions with non-zero weights can produce values close to zero in some points of space and so they won't erode the probability for this class.
This gives us a clue to the second less obvious effect of scaling the weights: since short weight vectors with lengths close to zero don't allow substantial variation in values of exponential functions the greater variation in inputs <em>x</em> is required then for getting high probabilities. In other words <span class="important">shorter weight vectors make the space less sensitive to the changes in the input vectors and vice versa</span>. And it is important to note here that shortening only a single weight vector makes less sensitive not only its own class area but actually <span class="tooltip">the whole space</span>.
<ins>It can be seen in <span class="scroll" data-target="#img10">figure 8</span> again when \(w_2\) becomes short not only the <span class="cby">yellow area</span> becomes more transparent but also the <span class="cbr">red</span> and <span class="cbg">green</span> areas. This happens because it is harder for the scalar product for the nearly zeroed-out weight to produce not only high positive values but also high negative values, which are required to gain high probabilities in other clean areas</ins>
</p>
</div>


<div class="col">
<p>
Overall, both biases and scaling effects can be generalized under the following rule:
</p>
<p class="important">
In a softmax layer any dimensionality-keeping <i>affine</i> transformation of the weight vectors can be replaced by another dimensionality-keeping <i>affine</i> transformation of the inputs
</p>
<p>
The proof for this fact is similar to the one we used for the scaling effect. Indeed, if you look at the system \eqref{ref104} again, you can notice that the scaling factor <em>s</em> can be replaced by some transformation matrix <em>T</em> instead, but the number of equations and unknown variables won't change - so a <span class="tooltip">satisfying replacement matrix <em>S</em></span>
<ins>It needs to be clarified that when we talked about the scaling effect we used a simplified version of affine transformation without translations allowed, because it wasn't needed there. However, here and generaly translations are allowed and for that the matrices <em>T</em> and <em>S</em> should be specified in <i>homogeneous</i> coordinates as well as vector <em>x</em> get an additional constant coordinate and <em>W</em> and <em>b</em> be united. Nevertheless, it also doesn't affect the logic behind the proof.</ins>
still exists when \(N\leq n+1\).
</p>

<p class="wide">
Another briefly mentioned before and probably overall undesirable property of softmax layer is that <span class="important">a high number of possible classes <em>N</em> leads to large volumes of <span class="tooltip">mixed noisy distributions</span><ins>by this we mean <b>dirty</b> areas where not a single probability exceeds 50%. In most classification tasks such points do not make much sense, because we want to make a mapping between a data sample and its single class, but not, say, 30% for one class and 10% for all other classes.</ins> in the class space</span>.
Imagine a NN has to classify data among 100 possible classes, then there are only \(2^{100}-1\) combinations of distributions with certain classes having equal and dominating probability, while only 100 of those combinations are useful for the task.
It may seem not so formidable as the space is infinite and the clean areas are also infinite, however, in practice distant points may become unachievable because inputs tend to be concentrated around the center due to permutations in the previous array of layers and bounding activations.
But having <span class="tooltip">longer weight vectors</span><ins>which as we have mentioned already increases sensitivity of the space</ins> and a large dimensionality of the inputs <em>n</em> tackles this problem.
<span class="scroll" data-target="#img11">Figure 9</span> demonstrates that an increase in the number of classes allocates <span class="tooltip">more space</span><ins>The transparent area in the center becomes bigger and 2 additional white stripes between clean areas appear. Overall contours become pale, which means that a greater percentage of the space is given to points with 50-70% per class, which is also an undesirable output</ins> to noisy areas compared to our basic example <span class="scroll" data-target="#img7">(fig. 5)</span> with the same dimensionality <em>n</em>.
<div class="img" id="img11"><div style="padding-bottom: 100%;">
<object data="a1_w5.svg" type="image/svg+xml"><img src="a1_w5.svg" alt="softmax space with 5 classes" /></object>
</div>
<p>figure 9 - class space for <em>N=5</em> and <em>n=2</em></p>
<pre></pre>
</div>
</p>
<p>
On the other hand, when <em>N>n+1</em> not all probability distributions <span class="tooltip">become achievable</span><ins>The formal proof for this fact can be found in the <b>Theorem 2</b> of „Sigsoftmax: Reanalysis of the Softmax Bottleneck”</ins> generally<span class="ref">[6]</span>.
For instance, in <span class="scroll" data-target="#img11">figure 9</span> you won't be able to find a point with the \({\{33\%\;33\%\;33\%\approx\!0\%\approx\!0\%\}}\) distribution, while in the basic example <span class="scroll" data-target="#img7">(fig. 5)</span> all combinations existed.
</p>
</div>





<h4>Convexity of the Class Space</h4>
<div class="col">
<p>
As you might have noticed from the pictures above, the contours of the softmax function form a convex set, in a sense that if we have 2 <span class="tooltip">points</span><ins>which are activations of the previous layer in our context</ins> <span class="cr">A</span> and <span class="cb">B</span> which belong to one class with a certain probability <em class="co">p</em>, then any point on the line segment between <span class="cr">A</span> and <span class="cb">B</span> also belongs to the same class with the same or higher probability <span class="scroll" data-target="#img5">(fig. 10)</span>.
<div class="img" id="img5"><div style="padding-bottom: 100%">
<object data="a1_soft_convexity.svg" type="image/svg+xml"><img src="a1_soft_convexity.svg" alt="softmax convexity" /></object>
</div>
<P>figure 10 - contours of softmax form a convex space</p>
<pre></pre>
</div>
It appears that this property stays true not only in our particular example but actually just for <span class="tooltip">any softmax layer</span><ins>any weight matrix values, number of classes and dimensionality of inputs</ins> configuration.
More strictly and generally speaking the above statement can be formulated in the following way:
</p>
<p class="important">
All points where softmax function for a certain class <em>i</em> yields a greater or equal probability than some threshold probability <em class="co">p</em> form a convex set:
$$ \begin{equation} \begin{split} &\mathbf{X}=\left \{ x\in\mathbb{R}^n:\:\:\: \mathrm{softmax}_{i}(x)\equiv \frac{e^{w_i\cdot x + w_{i,0}}}{\sum_{j=1}^{N} e^{w_j\cdot x + w_{j,0}}}\geq \co{p}\right\} \\&\Rightarrow\mathbf{X}\text{ - convex set} \end{split} \end{equation}\label{ref1} $$
</p>
<p>
It is an important property for us increasingly because in a real NN with many layers and weights we can take any set of training examples which belong to one class and then we know that if after training NN we calculate their respective activations in the penultimate layer and think of any point, which lie <span class="tooltip">in between</span><ins>meaning any weighted average of these activations or in other words points from the convex hull formed by the activations</ins> these activations, then this point also belongs to the same class.
In a NN with a high dimensionality of activations in the last but one layer these points "in between" can be relatively distant from each other and occupy huge volumes and still belong to the same class.
We can think of different ways how to make use of these points "in between". For instance, we can add some extra conditions to the cost function regarding these points to try to improve the performance of the NN or we can take a random point "in between" and use it as a latent variable for a generative NN, which is supposed to produce output similar to our data (a <span class="tooltip">related</span><ins>in that model a normally distributed noise is added to the activations though instead of taking random points from the convex hull formed by these activations, but it is a similar approach</ins> example of such generative model can be found in the paper by DP.Kingma et al.<span class="ref">[2]</span>).
</p>
</div>

<div class="fold">
<div>Formal Proof<pre><div></div><div></div></pre></div>
<div>
<p>Note that we omit biases from this proof for simplicity and because they don't affect any stages of the proof. A set is convex if any 2 random points <em class="cr">A</em> and <em class="cb">B</em> from the set when connected form a line segment, which also belongs to the same set. For our particular case it can be stated as
$${\forall \cr{A},\cb{B} \in\mathbf{X}\subset\mathbb{R}^n:}\; {\frac{e^{w_i\cdot \cr{A}}}{\sum_{j=1}^{N} e^{w_j\cdot \cr{A}}}\geq \co{p}}\;\; \text{and}\;\; {\frac{e^{w_i\cdot \cb{B}}}{\sum_{j=1}^{N} e^{w_j\cdot \cb{B}}}\geq \co{p}}\\ \text{then}\; {\frac{e^{w_i\cdot (t\cr{A}+(1-t)\cb{B})}}{\sum_{j=1}^{N} e^{w_j\cdot (t\cr{A}+(1-t)\cb{B})}}\geq \co{p},}\;\;{t\in[0,1]}\:\: \Leftrightarrow \:\: \mathbf{X}\text{ - convex set}$$
</p>
<p>So given first 2 inequalities, we need to prove that the third one stays true when <span class="tooltip">\(t\in[0,1]\)</span><ins>such <em>t</em> defines all the points on the line segment connecting initial points <em class="cr">A</em> and <em class="cb">B</em></ins>.
First, we simplify the <span class="tooltip">given inequalities</span><ins>showing this only for the first inequality but the same applies to the other 2</ins> taking in account that all exponential functions are always greater than zero:
$$\bullet\quad{\frac{e^{w_i\cdot \cr{A}}}{\sum_{j=1}^{N} e^{w_j\cdot \cr{A}}}\geq \co{p}}\\
\bullet\quad{\frac{1}{1+\frac{\sum_{j\neq i} e^{w_j\cdot \cr{A}}}{e^{w_i\cdot \cr{A}}}}\geq \co{p}}\quad\text{(both sides >0)}\\
\bullet\quad{1+\sum_{j\neq i} e^{w_j\cdot \cr{A}-w_i\cdot \cr{A}}\leq \frac{1}{\co{p}}}$$
$$\circ\quad\begin{equation}{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cr{A}}\leq \frac{1-\co{p}}{\co{p}}}\end{equation}\label{ref2}$$
</p>
<p>And for the third inequality concerning the line segment points written as \eqref{ref2} we will need a minor rearrangement:
$$
\bullet\quad{\sum_{j\neq i} e^{(w_j-w_i)\cdot (t\cr{A}+(1-t)\cb{B})}\leq \frac{1-\co{p}}{\co{p}}}\\
\circ\quad{\sum_{j\neq i} e^{(w_j-w_i)\cdot (\cr{A}-\cb{B})t}e^{(w_j-w_i)\cdot \cb{B}}\leq \frac{1-\co{p}}{\co{p}}}
$$
</p>
<p>Since we need to prove that the last inequality above stays true, let's for now assume the contrary:
$$
\begin{equation}{\sum_{j\neq i} e^{(w_j-w_i)\cdot (\cr{A}-\cb{B})t}e^{(w_j-w_i)\cdot \cb{B}}\gt \frac{1-\co{p}}{\co{p}}}\end{equation}\label{ref3}
$$</p>
<p>
Assuming that \eqref{ref3} is true we continue by <span class="tooltip">uniting</span><ins>since the term on the right side of the inequalities is the same</ins> it with our 2 other given inequalities:
$$
{\sum_{j\neq i} e^{(w_j-w_i)\cdot (\cr{A}-\cb{B})t}e^{(w_j-w_i)\cdot \cb{B}}}\:{\gt\sum_{j\neq i} e^{(w_j-w_i)\cdot \cr{A}}}\\
{\sum_{j\neq i} e^{(w_j-w_i)\cdot (\cr{A}-\cb{B})t}e^{(w_j-w_i)\cdot \cb{B}}}\:{\gt\sum_{j\neq i} e^{(w_j-w_i)\cdot \cb{B}}}
$$
Let's divide the second inequality by the right side's expretion (again taking in account that exponential functions >0):
$$
{\frac{\sum_{j\neq i} \left (e^{(w_j-w_i)\cdot (\cr{A}-\cb{B})}\right )^t e^{(w_j-w_i)\cdot \cb{B}}}{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cb{B}}}\gt 1}
$$
</p>
<p>
Now note that expretion in the wide brackets is a function of the form \(f(x)=x^t\), with <em>x>0</em> and \(t\in[0,1]\). With such <em>t</em> these functions are <span class="tooltip">concave</span><ins>consider \(\sqrt{x}\) for instance</ins>.
Then we can use <i>Jensen's inequality</i><span class="ref">[3]</span> in the finite form for a concave function, which states that \({f\left (\frac{\sum x_j a_j}{\sum a_j} \right )} \geq {\frac{\sum f(x_j) a_j}{\sum a_j}} \) to derive stronger but simplier inequality from the last inequality above:
$$
\bullet\quad {\left (\frac{\sum_{j\neq i} e^{(w_j-w_i)\cdot (\cr{A}-\cb{B})} e^{(w_j-w_i)\cdot \cb{B}}}{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cb{B}}}\right )^t} \geq {\frac{\sum_{j\neq i} \left (e^{(w_j-w_i)\cdot (\cr{A}-\cb{B})}\right )^t e^{(w_j-w_i)\cdot \cb{B}}}{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cb{B}}}\gt 1}\\
\bullet\quad {\left (\frac{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cr{A}} }{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cb{B}}}\right )^t \gt 1}\quad (\frac{1}{t}\in[1,+\infty)\;\text{ so }x^\frac{1}{t}\text{ is monotonically increasing} )
$$
$$\circ\quad \begin{equation}{\frac{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cr{A}} }{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cb{B}}} \gt 1}\end{equation}\label{ref4}
$$
</p>
<p>Good, let's work with the other inequality then:
$$
\bullet\quad {\sum_{j\neq i} e^{(w_j-w_i)\cdot (\cr{A}-\cb{B})t}e^{(w_j-w_i)\cdot \cb{B}}}\:{\gt\sum_{j\neq i} e^{(w_j-w_i)\cdot \cr{A}}}\\
\bullet\quad {\frac{\sum_{j\neq i} \left (e^{(w_j-w_i)\cdot (\cr{A}-\cb{B})}\right )^t e^{(w_j-w_i)\cdot \cb{B}}}{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cb{B}}}}\:{\gt\frac{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cr{A}}}{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cb{B}}}}\quad\text{(Jensen's Inequality again)}\\
\bullet\quad {\left (\frac{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cr{A}} }{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cb{B}}}\right )^t}\:{\gt\frac{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cr{A}}}{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cb{B}}}}\\
\bullet\quad {\left (\frac{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cr{A}} }{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cb{B}}}\right )^{t-1}\gt 1}\quad (\frac{1}{t-1}\in(-\infty,-1]\;\text{ so }x^\frac{1}{t-1}\text{ is monotonically decreasing when } x>0 )
$$
$$\circ\quad \begin{equation}{\frac{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cr{A}} }{\sum_{j\neq i} e^{(w_j-w_i)\cdot \cb{B}}} \lt 1}\end{equation}\label{ref5}
$$
</p>
<p>As you can see \eqref{ref4} and \eqref{ref5} contradict each other, so our assumption in \eqref{ref3} was wrong and, therefore, \({\sum_{j\neq i} e^{(w_j-w_i)\cdot (\cr{A}-\cb{B})t}e^{(w_j-w_i)\cdot \cb{B}}}\leq{\frac{1-\co{p}}{\co{p}}}\), which concludes the proof. \(\square\)

</p>
</div>
</div>

<p>
It is worth noting that the same property can be formulated simply as that <span class="important">softmax for a certain class <em>i</em> is a concave <span class="tooltip">function</span><ins>as a function of activations of the previous layer but not the whole non-linear neural network</ins></span>. Indeed, inequality in \eqref{ref1} also defines <i>the set under the graph of softmax function</i>, which we have proven to be convex and this is the only requirement for the function to be concave.
</p>
<p>
Lastly, For the alternative classification approach with linear Support Vector Machines the convexity property of the class zones also remains true.
</p>


<table class="refblock">
<tr><td>1.</td><td>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean - <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a> [NIPS] (2013)</td></tr>
<tr><td>2.</td><td>Diederik P. Kingma, Danilo J. Rezendey, Shakir Mohamedy, Max Welling - <a href="https://arxiv.org/pdf/1406.5298.pdf" target="_blank">Semi-supervised Learning with Deep Generative Models</a> [NIPS] (2014)</td></tr>
<tr><td>3.</td><td>Wikipedia - <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" target="_blank">Jensen's Inequality</a> (as of 2019)</td></tr>
<tr><td>4.</td><td>Yoshua Bengio, Aaron Courville, Pascal Vincent - <a href="https://arxiv.org/pdf/1206.5538.pdf" target="_blank">Representation Learning: A Review and New Perspectives</a> [IEEE Transactions on Pattern Analysis and Machine Intelligence 35, section 3.4] (2013)</td></tr>
<tr><td>5.</td><td>Yichuan Tang - <a href="https://arxiv.org/pdf/1306.0239v4.pdf" target="_blank">Deep Learning using Linear Support Vector Machines</a> (2015)</td></tr>
<tr><td>6.</td><td>Sekitoshi Kanai, Yasuhiro Fujiwara, Yuki Yamanaka, Shuichi Adachi - <a href="https://arxiv.org/pdf/1805.10829.pdf" target="_blank">Sigsoftmax: Reanalysis of the Softmax Bottleneck</a> [NIPS] (2018)</td></tr>
</table>


<div id="remark42"></div>
<script type="text/javascript" src="/com/remark42_load.js"></script>


	</div>
	<div id="img_overlay"><div></div><p></p><pre></pre></div>
	<div id="back_arrow"><div></div></div>
	<div id="footer">
		&gt; Powered by 
		<a href="https://jquery.com/" target="_blank"><img src="/com/logo_jquery.png" alt="JQuery" style="vertical-align: bottom"></a>
		<a href="https://www.mathjax.org/" target="_blank"><img src="/com/logo_mathjax.png" alt="MathJax"></a>
		<a href="https://pytorch.org/" target="_blank"><img src="/com/logo_pytorch.png" alt="PyTorch" style="vertical-align: bottom; height: 1.2em"></a>
		<a href="https://www.wolfram.com/mathematica/" target="_blank"><img src="/com/logo_mathematica.png" alt="Wolfram Mathematica" style="height: 1.3em"></a>
		<a href="https://stephanwagner.me/jBox" target="_blank"><img src="/com/logo_jbox.png" alt="jBox" style="height: 1.2em"></a>
		<a href="https://remark42.com" target="_blank"><img src="/com/logo_remark42.png" alt="Remark42" style="height: 1.2em"></a>
		<br>
		&gt; This site doesn't use cookies ¯\_(ツ)_/¯
	</div>
	<script type="text/javascript" src="/com/main.js"></script>
  </body>
</html>